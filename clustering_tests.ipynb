{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "urban-puzzle",
   "metadata": {},
   "source": [
    "# Clustering of websites by text\n",
    "\n",
    "Thanks to the scrapping notebook I could gather the text of the given websites and the links in the homepage and write it into a text file. \n",
    "\n",
    "There is one text file per site. \n",
    "\n",
    "While most of the 50 sites were scrapped succesfully, there a few I couldn't get. \n",
    "\n",
    "Out of 50 sites:\n",
    "\n",
    "2 couldn't be requested properly.\n",
    "5 gave a 403 or 503 http error code. \n",
    "1 was requested, but the script recovered the text of the loading page. \n",
    "\n",
    "I will start the clustering with the data I could gather, and go back to the missing websites after the clustering process is well stablished."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-aerospace",
   "metadata": {},
   "source": [
    "My first step is to load the text data and load it into a dataframe, in order to start exploring it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mysterious-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "established-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the contents directory\n",
    "CONTENTS_DIR = './site_contents/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "democratic-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all file names from the directory.\n",
    "\n",
    "file_names = [file for file in os.listdir(CONTENTS_DIR)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "coated-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text of each file\n",
    "file_contents = []\n",
    "\n",
    "for name in file_names:\n",
    "    \n",
    "    with open(CONTENTS_DIR + name, 'r') as content:\n",
    "        site_text = content.read()\n",
    "    \n",
    "    file_contents.append(site_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "political-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataframe to store the site names and the extracter text\n",
    "websites_df = pd.DataFrame({'site': map(lambda name: name.replace('.txt','' ), file_names),\n",
    "                            'raw_text': file_contents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "younger-gathering",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43 entries, 0 to 42\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   site      43 non-null     object\n",
      " 1   raw_text  43 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 816.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# Some info on the dataframe.\n",
    "\n",
    "websites_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-tiger",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-canal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-relative",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "websites_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-sperm",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which languages are have stopwords list in nltk?\n",
    "print(f'There are a total of {len(stopwords.fileids())} languages: \\n')\n",
    "print(', '.join(stopwords.fileids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-poison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build list of all stopwords:\n",
    "\n",
    "stop_words = []\n",
    "for lan in stopwords.fileids():\n",
    "    stop_words.extend(stopwords.words(lan))\n",
    "\n",
    "print(f'Gathered a total of {len(stop_words)} stopwords.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-israeli",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of curated html tags that may have remained.\n",
    "# The list contains all html tags, and the ones that shouldn't be included\n",
    "# as stopwords are preceded by the simbol '#'.\n",
    "\n",
    "with open('filter_tags', 'r') as tags:\n",
    "    stop_words.extend([tag.strip() for tag in tags.readlines() if (not tag.startswith('#') and len(tag.strip())>=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add to the stopwords a list of file types and html tags, in case something got through\n",
    "\n",
    "file_types = ['pdf', 'jpg', 'jpeg', 'png', 'gif', 'exe', 'js', 'zip', 'tar', 'gz', '7z', 'rar']\n",
    "\n",
    "stop_words.extend(file_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set(stop_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cardiovascular-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "websites_df['raw_text'] = websites_df['raw_text'].apply(lambda raw: re.sub(r'\\b\\d+\\b', '',raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "seventh-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column with split text, and one with the length of the split text.\n",
    "websites_df['wordcount']=websites_df['raw_text'].apply(lambda mytext: len(mytext.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "breeding-queen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wordcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>15707.116279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>23752.146283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5683.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10214.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16095.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>146517.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           wordcount\n",
       "count      43.000000\n",
       "mean    15707.116279\n",
       "std     23752.146283\n",
       "min         0.000000\n",
       "25%      5683.000000\n",
       "50%     10214.000000\n",
       "75%     16095.500000\n",
       "max    146517.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "websites_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-complaint",
   "metadata": {},
   "outputs": [],
   "source": [
    "websites_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-jefferson",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Some general information on the wordcounts.\n",
    "websites_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-gathering",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "websites_df.sort_values('wordcount').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "websites_df.drop(websites_df[websites_df['wordcount']< 100].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "websites_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see how the wordcount is distributed.\n",
    "\n",
    "websites_df['wordcount'].apply(np.log10).hist(bins = 20, figsize=(7,7))\n",
    "plt.title('Word count distribution')\n",
    "plt.xlabel('Log10(wordcount)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-september",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Viewing the 10 websites with the lowest wordcounts.\n",
    "\n",
    "websites_df.sort_values('wordcount').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-tourist",
   "metadata": {},
   "source": [
    "We can also see the sites with higher wordcounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "websites_df.sort_values('wordcount').tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-craps",
   "metadata": {},
   "source": [
    "From the table above we see that there are a few websites with very few words, and that they correspond mostly to failures in getting the content of the page. We should get rid of these sites, and a good way is to drop those with a low wordcount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-mixture",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set minimun amount of word required to remain in the data set. \n",
    "MIN_WORDS = 100\n",
    "\n",
    "# Drop sites with less words than MIN_WORDS.\n",
    "websites_df.drop(websites_df[ websites_df['wordcount'] < MIN_WORDS ].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what we have left.\n",
    "\n",
    "websites_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-printing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the 10 websites with the lowest wordcounts after drop.\n",
    "\n",
    "websites_df.sort_values('wordcount').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see how the wordcount is distributed.\n",
    "\n",
    "websites_df['wordcount'].apply(np.log10).hist(bins = 20, figsize=(7,7))\n",
    "plt.title('Word count distribution')\n",
    "plt.xlabel('Log10(wordcount)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-symbol",
   "metadata": {},
   "source": [
    "We still have very different lenghts, but even a few hundred words could help us classify a site. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-duration",
   "metadata": {},
   "source": [
    "Before tokenizing and vectorizing, it would be good some things that are likely to appear many times but not give us a lot of information. Since we are dealing with different languages, some language-specific pre-processing techiniques like removing stopwords or lematization would be harder to implement. \n",
    "\n",
    "Some things that can appear in the websites and won't give much info. \n",
    "* Pure numbers.\n",
    "* Pure symbols like +, - ?, &, etc. (Beware of not removing non-latin alphabet's characters. )\n",
    "* email adresses and URLs. \n",
    "\n",
    "The idea behind removing these elements is the intuition that sites will be clustered by language first, and then perhaps by topic within each language. The information relative to these elemenets most likely doesn't depend on the above elements. \n",
    "\n",
    "If I assume I know at least what some of the languages are, I can remove some of the stopwords. But this assumes knowledge that I'm not sure I have available. I will leave this aside for the time being, but it could be dealt with on a future version of the process.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-watts",
   "metadata": {},
   "source": [
    "## Vectorizing text with Tf-Idf\n",
    "\n",
    "I will use Tf-Idf to vectorize my texts. This will build a vocabulary, and assing a vector to each document. The i-th coordinate of the vector is the product between the Term Frequency and the Inverse Document Frecuency of the i-th word in the vocabulary.\n",
    "\n",
    "Furthermore, this vectorizer will normalize all vectors to length 1, which will reduce the efect of comparing texts of very different lengths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-pointer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a vectorizer object. We can pass parameter to control certain aspects\n",
    "# of the vectorization process. For now we use the default values. \n",
    "\n",
    "# Changed token patterns to keep words with 3 or more letters only.\n",
    "vectorizer = TfidfVectorizer(min_df =2,\n",
    "                             max_df = 0.5,\n",
    "                             stop_words=stop_words,\n",
    "                             token_pattern = '(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b', ngram_range=(1,2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-passage",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The default regexp select tokens of 2 or more alphanumeric characters \n",
    "(punctuation is completely ignored and always treated as a token separator).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-fraction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Building vocabulary and vectors\n",
    "\n",
    "doc_vectors = vectorizer.fit_transform(websites_df['raw_text'])\n",
    "\n",
    "print(f'We obtained a vocabulary of {len(vectorizer.vocabulary_)} different words and bigrams.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-death",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'We identified {len(vectorizer.stop_words_)} words or bigrams that appear in only one document, or in more than half' )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.stop_words_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-birmingham",
   "metadata": {},
   "source": [
    "The vectors obtained in this way are stored in a sparse matrix (i.e. it's entries are mostly 0).\n",
    "\n",
    "Each row corresponds to a document, and each column to a word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-circular",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'The dimensions of the document matrix are {doc_vectors.shape} .')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-singles",
   "metadata": {},
   "source": [
    "###  Visualizing the document vectors\n",
    "\n",
    "With the documents matrix built, we can explore it superficially in order to get an idea of the structures of our documents. \n",
    "\n",
    "We show two visualizations: The non-zero elements of the matrix, and the cosine similarity matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-endorsement",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualizing the non-zero elements of the matrix. \n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.spy(doc_vectors, markersize=1, aspect = 'auto')\n",
    "plt.title('Document matrix non-zero elements')\n",
    "plt.xlabel('Word in vocabulary')\n",
    "plt.ylabel('Document number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-median",
   "metadata": {},
   "source": [
    "From this plot we can see there are two documents (numbers 13 and 32) containtn a lot of words (they look like horizontal lines) and one (number 9) with very few. The rest of them seem mostly balanced, but in any case shouldn't be a major factor. \n",
    "\n",
    "From this bird's eye view of our data we cannont distinguish any clear cluster structure.\n",
    "This may seem suprising, as one might expect that at least sites would naturally be clustered by language. However, words in the vocabulary are not grouped by language, hiding this effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-trace",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Computing cosine similarities. Since verctors are normalized to 1, \n",
    "# it suffices to multiply the matrix by its transpose. \n",
    "\n",
    "cosine_sims = doc_vectors * doc_vectors.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-australia",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Heatmap of the cosine similarities. \n",
    "# There are no correletions higher than 0.6 (other than the diagonal), \n",
    "# So we choose this value for a cutoff of the scale.\n",
    "\n",
    "my_cmap = cm.get_cmap('viridis')\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cosine_sims.toarray(), cmap=my_cmap, vmax=.8)\n",
    "plt.colorbar(shrink = 0.8)\n",
    "\n",
    "plt.title('Cosine Similarities between documents')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-moore",
   "metadata": {},
   "source": [
    "##  Dimensional reduction with PCA\n",
    "\n",
    "Reducing dimensionality can help the KMeans algorithm work better, as it relies on euclidean distances. \n",
    "\n",
    "The price to pay for this is that we may loose some interpretability, as the components of our dimensions will be mixed. \n",
    "\n",
    "Let's try ir out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-cooperative",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep the 2 principal components, and visualize the data to see if we see something.\n",
    "# I think this is a long shot.\n",
    "\n",
    "# Set random state for reproducibility.\n",
    "decomposer_2d = TruncatedSVD(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_2d = decomposer_2d.fit_transform(norm_docs)\n",
    "decomposer_2d.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-ottawa",
   "metadata": {},
   "source": [
    "We can get the 10 most important words for our principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-billy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert the vocabulary dictionary\n",
    "inv_vocab = {v:k for k,v in vectorizer.vocabulary_.items()}\n",
    "\n",
    "# Sort component arguments by descending order of the components values.\n",
    "\n",
    "word_ids_1 = np.argsort(decomposer_2d.components_[0,:])[::-1]\n",
    "word_ids_2 = np.argsort(decomposer_2d.components_[1,:])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-british",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, ids in enumerate([word_ids_1, word_ids_2]):\n",
    "    words = [inv_vocab[w_id] for w_id in ids[:10]] \n",
    "    \n",
    "    print(f'Words of dimension {i} :\\n')\n",
    "    print(' | '.join(words))\n",
    "    print('\\n'+'='*20+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(decomposer_2d.components_[0,:])\n",
    "plt.plot(decomposer_2d.components_[1,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-boost",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "plt.scatter(docs_2d[:,0], docs_2d[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-fitting",
   "metadata": {},
   "source": [
    "###  Keeping 10 principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposer_10 = TruncatedSVD(n_components = 10)\n",
    "\n",
    "decomposer_10.fit(doc_vectors)\n",
    "docs_10d = decomposer_10.transform(doc_vectors)\n",
    "\n",
    "decomposer_10.singular_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposer_10.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-supplier",
   "metadata": {},
   "source": [
    "###  Keeping the 100 first principa components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposer_100 = TruncatedSVD(n_components = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposer_100.fit(doc_vectors)\n",
    "docs_100d = decomposer_100.transform(doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-shell",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decomposer_100.singular_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(decomposer_100.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_100d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-speaker",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(decomposer_100.explained_variance_)\n",
    "plt.title('Explained variance of each component')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-buyer",
   "metadata": {},
   "source": [
    "Let's cluster using these reduced vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-salmon",
   "metadata": {},
   "source": [
    "##  Clustering using KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "\n",
    "KMeans_model = KMeans(n_clusters = 5, n_init=500, max_iter= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-perception",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find the clusters\n",
    "KMeans_model.fit(norm_docs)\n",
    "\n",
    "# Write the labels into the dataframe.\n",
    "websites_df['cluster_label'] =  KMeans_model.labels_\n",
    "\n",
    "# How many elements in each cluster?\n",
    "websites_df.groupby('cluster_label')['site'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see cosine simlarities ordered by cluster\n",
    "\n",
    "sorted_vecs = vectorizer.transform(websites_df.sort_values('cluster_label')['raw_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_indices = websites_df.sort_values('cluster_label')['cluster_label']\n",
    "n_sites = websites_df.shape[0]\n",
    "clabel = np.zeros((n_sites, n_sites), dtype = int) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-multiple",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(n_sites):\n",
    "    for j in range(n_sites):\n",
    "        if ordered_indices.iloc[i] == ordered_indices.iloc[j]:\n",
    "            clabel[i,j] = ordered_indices.iloc[i]\n",
    "            \n",
    "clabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-tractor",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sorted cosine sims\n",
    "sorted_sims = sorted_vecs * sorted_vecs.transpose()\n",
    "\n",
    "my_cmap = cm.get_cmap('viridis')\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(sorted_sims.toarray(), cmap=my_cmap, vmax=0.9)\n",
    "plt.colorbar(shrink = 0.8)\n",
    "\n",
    "for i in range(n_sites):\n",
    "    for j in range(n_sites):\n",
    "        if clabel[i, j] >=0:\n",
    "            text = plt.text(j, i, clabel[i, j],\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "plt.title('Cosine Similarities between documents')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-pickup",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-seafood",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x = docs_10d[:,0]\n",
    "y = docs_10d[:,1]\n",
    "z = docs_10d[:,2]\n",
    "\n",
    "\n",
    "\n",
    "ax.scatter(x, y, z, c=KMeans_model.labels_, marker='o')\n",
    "\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-contact",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "websites_df[websites_df['cluster_label']==4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_vocab = {v:k for k,v in vectorizer.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-hardware",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dict = {}\n",
    "for i in range(KMeans_model.cluster_centers_.shape[0]):\n",
    "    vec = KMeans_model.cluster_centers_[i]\n",
    "    argsorted = np.argsort(vec)[::-1]\n",
    "    \n",
    "    cluster_dict[f'{i}'] = [inv_vocab[i] for i in argsorted[:15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-indonesia",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k, v in cluster_dict.items():\n",
    "    print(f'Cluster {k} most important words \\n')\n",
    "    print(' | '.join(v))\n",
    "    print('\\n\\n'+'='*20 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-glass",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette should be close to 1\n",
    "# C-H should be as big as possible\n",
    "# D-B should be close to 0\n",
    "# Inertia should be minimized. \n",
    "\n",
    "scores_dict = {'sil': [],\n",
    "               'ch': [],\n",
    "               'db': [],\n",
    "               'inertia': []\n",
    "              }\n",
    "\n",
    "#doc_vec_array = doc_vectors.toarray()\n",
    "\n",
    "mydocs = doc_vectors.toarray()\n",
    "\n",
    "cnumbers = range(2,int(doc_vectors.shape[0]/3+1))\n",
    "for c_number in cnumbers:\n",
    "    \n",
    "    n_model = KMeans(n_clusters=c_number, n_init= 100, max_iter=100)\n",
    "    n_model.fit(mydocs)\n",
    "    labels = n_model.labels_\n",
    "    # Compute scores\n",
    "\n",
    "    model_scores = {'sil': silhouette_score(mydocs, labels),\n",
    "                    'ch': calinski_harabasz_score(mydocs, labels),\n",
    "                    'db': davies_bouldin_score(mydocs, labels),\n",
    "                    'inertia': n_model.inertia_}\n",
    "\n",
    "    for index_name, score in model_scores.items():\n",
    "        scores_dict[index_name].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-terror",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(cnumbers, scores_dict['inertia'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-locking",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(cnumbers, scores_dict['sil'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_mean = [np.sqrt(sil * ch) for sil, ch in zip(scores_dict['sil'], scores_dict['ch'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(geo_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cnumbers, scores_dict['ch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-channels",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cnumbers, scores_dict['db'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-louisiana",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-proportion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-correction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-salon",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names=['www.pelopincho.com.txt']\n",
    "file_contents = []\n",
    "\n",
    "for name in file_names:\n",
    "    \n",
    "    with open(CONTENTS_DIR + name, 'r') as content:\n",
    "        site_text = content.read()\n",
    "    \n",
    "    file_contents.append(site_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "pelopincho_df = pd.DataFrame({'site': map(lambda name: name.replace('.txt','' ), file_names),\n",
    "                            'raw_text': file_contents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "pelopincho_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-buffer",
   "metadata": {},
   "outputs": [],
   "source": [
    "pelopincho_df['raw_text'] = websites_df['raw_text'].apply(lambda raw: re.sub(r'\\b\\d+\\b', '',raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-event",
   "metadata": {},
   "outputs": [],
   "source": [
    "pelopincho_vec =vectorizer.transform(pelopincho_df['raw_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = KMeans_model.predict(pelopincho_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-geography",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
