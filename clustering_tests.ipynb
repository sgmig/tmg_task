{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bibliographic-honor",
   "metadata": {},
   "source": [
    "# Clustering of websites by text\n",
    "\n",
    "Thanks to the scrapping notebook I could gather the text of the given websites and the links in the homepage and write it into a text file. \n",
    "\n",
    "There is one text file per site. \n",
    "\n",
    "While most of the 50 sites were scrapped succesfully, there a few I couldn't get. \n",
    "\n",
    "Out of 50 sites:\n",
    "\n",
    "2 couldn't be requested properly.\n",
    "5 gave a 403 or 503 http error code. \n",
    "1 was requested, but the script recovered the text of the loading page. \n",
    "\n",
    "I will start the clustering with the data I could gather, and go back to the missing websites after the clustering process is well stablished."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-norfolk",
   "metadata": {},
   "source": [
    "My first step is to load the text data and load it into a dataframe, in order to start exploring it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the contents directory\n",
    "CONTENTS_DIR = './site_contents/'\n",
    "\n",
    "# Get all file names from the directory.\n",
    "\n",
    "file_names = [file for file in os.listdir(CONTENTS_DIR)]\n",
    "\n",
    "# Read the text of each file\n",
    "file_contents = []\n",
    "\n",
    "for name in file_names:\n",
    "    \n",
    "    with open(CONTENTS_DIR + name, 'r') as content:\n",
    "        site_text = content.read()\n",
    "    \n",
    "    file_contents.append(site_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-massage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataframe with the site names and the extracted text.\n",
    "websites_df = pd.DataFrame({'site': map(lambda name: name.replace('.txt','' ), file_names),\n",
    "                            'raw_text': file_contents})\n",
    "\n",
    "# Some info on the dataframe.\n",
    "\n",
    "websites_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-operations",
   "metadata": {},
   "source": [
    "We can have a look a the Dataframe to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-beauty",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "websites_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-belfast",
   "metadata": {},
   "source": [
    "We see that there is an entry with empty text. \n",
    "We also expect to find some entries with very few words, which may not be enough to perform a classification reliably. \n",
    "\n",
    "A way to filter out these cases is to remove all entries whose text is shorter than a defined cutoff. \n",
    "\n",
    "We add a wordcount column to the dataframe in order to get this information.\n",
    "\n",
    "Note that this wordcount is only a rough first approximation, as text has not been treated yet, and some words will be removed afterwards. Still, it is useful for filtering some of the entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column with split text, and one with the length of the split text.\n",
    "websites_df['wordcount']=websites_df['raw_text'].apply(lambda mytext: len(mytext.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-butler",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Statistics on the wordcount.\n",
    "print(websites_df['wordcount'].describe())\n",
    "# Lets see how the wordcount is distributed and plot an histogram.\n",
    "# I add +1 to the wordcount in order to avoid computing log(0).\n",
    "\n",
    "\n",
    "websites_df['wordcount'].apply(lambda wc: np.log10(wc +1)).hist(bins = 20, figsize=(7,7))\n",
    "plt.title('Word count distribution')\n",
    "plt.xlabel('Log10(wordcount)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-carry",
   "metadata": {},
   "source": [
    "Apart from 2 sites, most of our texts have more than 100 words. We can remove the oones that don't reach this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-animal",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Have a look at the shortest entries.\n",
    "websites_df.sort_values('wordcount').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-collapse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set minimum amount of words to remain in the dataset.\n",
    "\n",
    "min_words = 100\n",
    "\n",
    "# Drop short texts\n",
    "websites_df.drop(websites_df[websites_df['wordcount']< 100].index, inplace=True)\n",
    "\n",
    "websites_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-absence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see how the wordcount is distributed now.\n",
    "\n",
    "websites_df['wordcount'].apply(np.log10).hist(bins = 20, figsize=(7,7))\n",
    "plt.title('Word count distribution')\n",
    "plt.xlabel('Log10(wordcount)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'After removal, we have {websites_df.shape[0]} sites left.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-marshall",
   "metadata": {},
   "source": [
    "We still have very different lenghts, but even a few hundred words could help us classify a site. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-vault",
   "metadata": {},
   "source": [
    "## Vectorizing text with Tf-Idf\n",
    "\n",
    "I will use Tf-Idf to vectorize my texts. This will build a vocabulary, and assing a vector to each document. The i-th coordinate of the vector is the product between the Term Frequency and the Inverse Document Frecuency of the i-th word in the vocabulary.\n",
    "\n",
    "Furthermore, this vectorizer will normalize all vectors to length 1, which will reduce the efect of comparing texts of very different lengths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stopwords for all languages included in nltk\n",
    "print(f'Compile a list of stopwords from the {len(stopwords.fileids())} languages in nltk: \\n')\n",
    "print(', '.join(stopwords.fileids()))\n",
    "\n",
    "# Build list of all stopwords:\n",
    "\n",
    "stop_words = []\n",
    "for lan in stopwords.fileids():\n",
    "    stop_words.extend(stopwords.words(lan))\n",
    "\n",
    "# Get a list of curated html tags that may have remained.\n",
    "# The list contains all html tags, and the ones that shouldn't be included\n",
    "# as stopwords are preceded by the simbol '#'.\n",
    "\n",
    "print('\\nInclude curated HTML tags and common file extensions.\\n')\n",
    "with open('filter_tags', 'r') as tags:\n",
    "    stop_words.extend([tag.strip() for tag in tags.readlines() if (not tag.startswith('#') and len(tag.strip())>=3)])\n",
    "\n",
    "# Let's add to the stopwords a list of file types and html tags, in case something got through\n",
    "\n",
    "file_types = ['pdf', 'jpg', 'jpeg', 'png', 'gif', 'exe', 'js', 'zip', 'tar', 'gz', '7z', 'rar']\n",
    "\n",
    "stop_words.extend(file_types)\n",
    "\n",
    "# Remove repetitions\n",
    "\n",
    "stop_words = list(set(stop_words))\n",
    "print(f'Gathered a total of {len(stop_words)} unique stopwords.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers_lower(text):\n",
    "    \"\"\"\n",
    "    Preprocessing function to be passed to the tokenizer. \n",
    "    It will remove numbers that appear by themselves of between '-'.\n",
    "    This removes things such as telephone numbers. \n",
    "    It also lowecases the entirer string.\n",
    "    \n",
    "    text: string. Full text to be treated. \n",
    "    \"\"\"\n",
    "    \n",
    "    processed = re.sub(r'\\b\\d+\\b', '',text)\n",
    "    processed = processed.lower()\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a vectorizer object. We can pass parameter to control certain aspects\n",
    "# of the vectorization process.\n",
    "\n",
    "min_freq = 2 # parameter to remove very uncommon words.\n",
    "max_freq = 0.4 # parameter to remove too common words.\n",
    "\n",
    "# Changed token patterns to keep words with  or more letters only.\n",
    "min_letters = 3\n",
    "my_tokens = '(?u)\\\\b\\\\w{'+ str(min_letters -1) + '}\\\\w+\\\\b'\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = min_freq,\n",
    "                             max_df = max_freq,\n",
    "                             preprocessor= remove_numbers_lower,\n",
    "                             stop_words=stop_words,\n",
    "                             token_pattern = my_tokens,\n",
    "                             ngram_range=(1,1)\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-maria",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Building vocabulary and generating the document term matrix (dtm).\n",
    "\n",
    "dtm = vectorizer.fit_transform(websites_df['raw_text'])\n",
    "\n",
    "print(f'We obtained a vocabulary of {len(vectorizer.vocabulary_)} different words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-attachment",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'We identified {len(vectorizer.stop_words_)} words to be ignored based on'\n",
    "      + f' max_df= {max_freq} and min_df = {min_freq}' )\n",
    "\n",
    "#print('\\n\\n' + ' | '.join(vectorizer.stop_words_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-fourth",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build an inverse vocabulary dictionary to retrieve words easily. \n",
    "\n",
    "inv_vocab = {  w_id: word  for word, w_id in vectorizer.vocabulary_.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-replacement",
   "metadata": {},
   "source": [
    "The vectors obtained in this way are stored in a sparse matrix (i.e. it's entries are mostly 0).\n",
    "\n",
    "Each row corresponds to a document, and each column to a word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-timing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'The dimensions of the document matrix are {dtm.shape}'\n",
    "      + f', and it has {dtm.nnz} non-zero elements .')\n",
    "print(f'That represents {dtm.nnz * 100/np.multiply(*dtm.shape):.2f} percent of the entries')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-olympus",
   "metadata": {},
   "source": [
    "###  Visualizing the document vectors\n",
    "\n",
    "With the documents matrix built, we can explore it superficially in order to get an idea of the structures of our documents. \n",
    "\n",
    "We show two visualizations: The non-zero elements of the matrix, and the cosine similarity matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-snowboard",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualizing the non-zero elements of the matrix. \n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.spy(dtm, markersize=1, aspect = 'auto')\n",
    "plt.title('Document matrix non-zero elements')\n",
    "plt.xlabel('Word in vocabulary')\n",
    "plt.ylabel('Document number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-premises",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Computing cosine similarities. Since verctors are normalized to 1, \n",
    "# it suffices to multiply the matrix by its transpose. \n",
    "\n",
    "cosine_sims = dtm * dtm.transpose()\n",
    "\n",
    "# Heatmap of the cosine similarities. \n",
    "# There are no correletions higher than 0.6 (other than the diagonal), \n",
    "# So we choose this value for a cutoff of the scale.\n",
    "\n",
    "my_cmap = cm.get_cmap('viridis')\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(cosine_sims.toarray(), cmap=my_cmap, vmax=.8)\n",
    "plt.colorbar(shrink = 0.8)\n",
    "\n",
    "plt.title('Cosine Similarities between documents')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-career",
   "metadata": {},
   "source": [
    "Although clusters are hard to identify in this plot, we see that ther might be some relation between sites number 3,4 and 5. Let's check what they are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "websites_df.iloc[2:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-volleyball",
   "metadata": {},
   "source": [
    "The sites that seem correlated are:\n",
    "* de.jointeflons.com\n",
    "* de.plasticptfe.com\n",
    "* german.aiflon.com\n",
    "\n",
    "We can see that they are all german sites, and at least the first two are related to PTFE (teflon) products.\n",
    "\n",
    "This ilustrates that cosine similarity can capture some of the similarities between sites. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-reality",
   "metadata": {},
   "source": [
    "##  Dimensional reduction\n",
    "\n",
    "Reducing dimensionality can help the KMeans algorithm work better, as it relies on euclidean distances. \n",
    "\n",
    "The price to pay for this is that we may loose some interpretability, as the components of our dimensions will be mixed. \n",
    "\n",
    "The question that arises is deciding how many dimensions to keep. A common criterium is is to plot the singular values obtained (scree plot), and keep the dimensions corresponding to find the elbow in the plot. \n",
    "\n",
    "We will keep the first 20 principal components and see if the elbow becomes aparent. It should be enough.\n",
    "\n",
    "NOTE: I consider this number of dimensions as in hyperparameter, similar to the max and min frequencies passed to the vectorizer. Of course, with a list of 50 sites, this choice wont necesarily be representative for a general case. But with a more representative sample of sites the same procedure could be followed. \n",
    "\n",
    "Once we determine the number of dimensions to keep, we will then perform KMeans on the dimensionally reduced vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to use truncated SVD because we are dealing with an sparse matrix.\n",
    "\n",
    "# Create the SVD object. \n",
    "\n",
    "decomposer_20d = TruncatedSVD(20, n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-superior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SVD/LSA and get the transformed doc vectors. \n",
    "docs_20d = decomposer_20d.fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-example",
   "metadata": {},
   "source": [
    "Now we plot the singular values, and look for the elbow in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-registrar",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "singular_values = decomposer_20d.singular_values_\n",
    "plt.plot(singular_values)\n",
    "plt.grid(True)\n",
    "plt.xticks(list(range(len(singular_values))))\n",
    "plt.title('Scree plot - Singular Values')\n",
    "plt.xlabel('Dimension index')\n",
    "plt.ylabel('Singular Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-hundred",
   "metadata": {},
   "source": [
    "It appears that in this plot the elbow correspond to the 7th singular value (indices start from 0).\n",
    "\n",
    "But let's use the helper method to detect the elbow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_number = np.array(list(range(len(singular_values))))\n",
    "\n",
    "elbow_dim_number = ClusterDecissionHelper.elbow_finder(dimension_number, singular_values, plot=True )\n",
    "\n",
    "print(f'Found elbow for dim. with index {elbow_dim_number}.')\n",
    "print(f'We should truncate to {elbow_dim_number+1} dimensions! (remember, indices start from 0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-plaintiff",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "We can see what are the most relevant words for these 7 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-column",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_dict = {}\n",
    "\n",
    "for i in range(7):\n",
    "    pc = decomposer_20d.components_[i, :] #get principal component\n",
    "    \n",
    "    # Get arguments sorted in decreasing order. \n",
    "    argsorted = np.argsort(pc)[::-1]\n",
    "    \n",
    "    # Find words in vocabulary. \n",
    "    pc_dict[f'pc_{i}'] =  [inv_vocab[i] for i in argsorted[:10] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-emerald",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print words\n",
    "print(\"Principal components words orderd by importance. \\n\")\n",
    "for k,v in pc_dict.items():\n",
    "    print(f'Principal component {k} : \\n')\n",
    "    print(' | '.join(v) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-lighting",
   "metadata": {},
   "source": [
    "There seems to be some overlapping between the componenets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-distributor",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "Principal componenets can be used to visualize our data, plotting each point according to it's first two components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pc(vectors, components = [0,1], title = None, **kwargs):\n",
    "    \"\"\"\n",
    "    Scatter plot of the principal components specified by the columns.\n",
    "    vectors: array-like or sparse matrix of size (n_samples * n_components).\n",
    "    components: list or tuple with the two directions to take as x,y.\n",
    "    title: string. Plot title.\n",
    "    kwargs for plt.scatter.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8,8))\n",
    "    \n",
    "    col_x, col_y = components\n",
    "\n",
    "    x = vectors[:,col_x]\n",
    "    y = vectors[:,col_y]\n",
    "    plt.scatter(x,y, **kwargs)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    \n",
    "    plt.xlabel(f'Component {col_x}')\n",
    "    plt.ylabel(f'Component {col_y}')\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-pharmaceutical",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_pc(docs_20d[:,:2], title='Sites')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-canal",
   "metadata": {},
   "source": [
    "Points are more or less divided along the two directions, but there is no obvious clustering in sight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-kingdom",
   "metadata": {},
   "source": [
    "## Selecting K from silhouette score\n",
    "\n",
    "We perfom clustering with different number of clusters and compare silhouette scores. The number of clusters that gives the highest silhouette score is considered the best.\n",
    "The process is repeated several times, and we retain the number of clusters that provided a max silhouette more times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterDecissionHelper():\n",
    "    \"\"\"\n",
    "    Suggest and optimal number of clusters K for KMeans based on their scores.\n",
    "    \n",
    "    This class allows to perform KMeans with different numbers of clusters and compute \n",
    "    the silhouette score, inertia, or both. Repeats the process several times, and analize\n",
    "    the scores for the optimal K. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_clusters, min_clusters = 2,\n",
    "                 repeat = 5, metric = 'euclidean', **kwargs ):\n",
    "        \n",
    "        \"\"\"\n",
    "        kwargs: pass arguments to sklearn.cluster.KMeans()\n",
    "        min_clusters: int. Lower bound for the number of clusters. Default is 2, the minimum\n",
    "                   allowed value. \n",
    "        max_clusters: int. Upper bound for the number of clusters.\n",
    "                      Maximum allowed value is (number of vectors - 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.repeat = repeat\n",
    "        self.metric = metric\n",
    "        \n",
    "        self.KMeans_kwargs = kwargs\n",
    "        \n",
    "        self.cluster_numbers = list(range(min_clusters,max_clusters+1))\n",
    "        \n",
    "        self.sil_scores = None\n",
    "        self.inertia_scores = None\n",
    "        \n",
    "        \n",
    "    def compute_scores(self, dt_matrix,\n",
    "                       sil_score = False, inertia = False):\n",
    "    \n",
    "        \"\"\"\n",
    "        Computes arrays of the desired score, of size self.repeat x len(self.cluster_numbers)\n",
    "        It need to be told which method to use.\n",
    "\n",
    "\n",
    "        dt_matrix: array-like or sparse matrix. Document term matrix with docs to be clustered.\n",
    "        sil_score: bool. Compute Silhouette score. Default is False. \n",
    "        inertia: bool. Compute inertia. Default is False.    \n",
    "        \"\"\"\n",
    "\n",
    "        repeat = self.repeat\n",
    "        cluster_numbers = self.cluster_numbers\n",
    "        #metric = self.metric\n",
    "\n",
    "        doc_number = dt_matrix.shape[0]\n",
    "\n",
    "        # Build array to keep the results of each run\n",
    "        sil_array = np.zeros((repeat, len(cluster_numbers)))\n",
    "        inertia_array = np.zeros((repeat, len(cluster_numbers)))\n",
    "\n",
    "        for c_idx,c_number in enumerate(cluster_numbers):\n",
    "\n",
    "            model = KMeans(n_clusters=c_number, **self.KMeans_kwargs)\n",
    "\n",
    "            for repetition in range(repeat):\n",
    "                # Fit the model and get the labels\n",
    "                model.fit(dt_matrix)\n",
    "                labels = model.labels_\n",
    "\n",
    "                # Compute scores\n",
    "\n",
    "                if sil_score:\n",
    "                    sil_array[repetition, c_idx] = silhouette_score(dt_matrix, labels, metric = self.metric)\n",
    "                if inertia:\n",
    "                    inertia_array[repetition, c_idx] = model.inertia_\n",
    "\n",
    "        if sil_score:\n",
    "            self.sil_scores = sil_array \n",
    "        if inertia:\n",
    "            self.inertia_scores = inertia_array\n",
    "            \n",
    "    def get_K_from_silhouettes(self):\n",
    "        \"\"\" \n",
    "        Identifies optimal K from silhouette scores obtained with compute_scores()\n",
    "        If two or more values of K are tied, the smallest one is selected.\n",
    "        \"\"\"\n",
    "        \n",
    "        scores = self.sil_scores\n",
    "\n",
    "        # The -1 factor ensures that if two or more values tie, the smaller one\n",
    "        # is retained. \n",
    "        ranked_K_indices = np.argsort(-1*np.bincount(scores.argmax(axis=1)), kind='stable')\n",
    "\n",
    "        winning_K = self.cluster_numbers[ranked_K_indices[0]]\n",
    "        \n",
    "        self.best_K_sil = winning_K\n",
    "\n",
    "        return winning_K\n",
    "    \n",
    "    def plot_scores(self, method = 'sil'):\n",
    "        \"\"\"Convinience method for plotting scores obtained with compute_scores().\n",
    "        method: string. 'sil' for silhouettes, or 'in' for inertia. \n",
    "        \"\"\"\n",
    "\n",
    "        titles = {'sil': 'Silhouette score', 'in': 'Inertia' }\n",
    "\n",
    "        scores = {'sil': self.sil_scores, 'in':self.inertia_scores}\n",
    "        \n",
    "        score_arr = scores[method]\n",
    "        title = titles[method]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize = (8,8))\n",
    "\n",
    "        for i in range(self.repeat):\n",
    "            ax.plot(self.cluster_numbers, score_arr[i,:])\n",
    "            \n",
    "        plt.xticks(self.cluster_numbers)\n",
    "        plt.xlabel('Cluster Number')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title(f'{title} for {self.repeat} repetitions.')\n",
    "\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    @staticmethod\n",
    "    def elbow_finder(x,y, plot = False):\n",
    "        \"\"\"Finds an elbow in the graph of y=f(x) by finding the maximum distance between\n",
    "        f(x) and the line that joins (x_0, y_0) with (x_final, y_final).\n",
    "        Depends strongly on the noise.\n",
    "        x,y: One dimensional arrays of values.\n",
    "        \"\"\"\n",
    "        \n",
    "        # initial point\n",
    "        pi = np.array([x[0], y[0]])\n",
    "        \n",
    "        # final point\n",
    "        \n",
    "        pf = np.array([x[-1], y[-1]]) \n",
    "        \n",
    "        # unit vector parallell to the line\n",
    "        \n",
    "        n_vec = (pf - pi) / np.linalg.norm((pf-pi))\n",
    "        \n",
    "        distances = np.zeros(len(x))\n",
    "        \n",
    "        # I don't need the inital and final point. Distance is 0 by definition.\n",
    "        for i in range(len(x[1:])):\n",
    "            \n",
    "            q =np.array([x[i],y[i]])\n",
    "            \n",
    "            dif_vec= pi-q\n",
    "            \n",
    "            d = np.linalg.norm( (dif_vec) - np.dot(dif_vec,n_vec) * n_vec )\n",
    "            \n",
    "            distances[i] = d\n",
    "            \n",
    "            elbow_x_id = distances.argsort(kind='stable')[::-1][0]\n",
    "            \n",
    "            elbow_x = x[elbow_x_id]\n",
    "            \n",
    "        if plot:\n",
    "            \n",
    "            plt.figure(figsize=(8,8))\n",
    "            plt.plot(x, y)\n",
    "            plt.plot(x[[0,-1]], y[[0,-1]], c = 'g')\n",
    "            plt.axvline(elbow_x, c='r')\n",
    "            \n",
    "        return elbow_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-sunrise",
   "metadata": {},
   "source": [
    "## KMeans Clustering\n",
    "\n",
    "We begin by building our 7 dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_reducer = TruncatedSVD(n_components = 7, n_iter= 10 )\n",
    "\n",
    "reduced_dtm =  dim_reducer.fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-solomon",
   "metadata": {},
   "source": [
    "Select K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "decider = ClusterDecissionHelper(max_clusters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-threat",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decider.compute_scores(reduced_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "decider.compute_scores(reduced_dtm, sil_score=False, inertia=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "decider.plot_scores('in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-petite",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decider.plot_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-vegetation",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_K = decider.get_K_from_silhouettes()\n",
    "\n",
    "print(f'The best value of K found was K = {best_K} .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model for the best K\n",
    "\n",
    "KMeans_model = KMeans(n_clusters = best_K, n_init=20, max_iter= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-crack",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find the clusters\n",
    "KMeans_model.fit(reduced_dtm)\n",
    "\n",
    "# Write the labels into the dataframe.\n",
    "websites_df['cluster_label'] =  KMeans_model.labels_\n",
    "\n",
    "# How many elements in each cluster?\n",
    "websites_df.groupby('cluster_label')['site'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see cosine simlarities ordered by cluster\n",
    "\n",
    "sorted_vecs = vectorizer.transform(websites_df.sort_values('cluster_label')['raw_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_indices = websites_df.sort_values('cluster_label')['cluster_label']\n",
    "n_sites = websites_df.shape[0]\n",
    "clabel = np.zeros((n_sites, n_sites), dtype = int) -1\n",
    "\n",
    "for i in range(n_sites):\n",
    "    for j in range(n_sites):\n",
    "        if ordered_indices.iloc[i] == ordered_indices.iloc[j]:\n",
    "            clabel[i,j] = ordered_indices.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-chapter",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sorted cosine sims\n",
    "sorted_sims = sorted_vecs * sorted_vecs.transpose()\n",
    "\n",
    "my_cmap = cm.get_cmap('viridis')\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(sorted_sims.toarray(), cmap=my_cmap, vmax=0.6)\n",
    "plt.colorbar(shrink = 0.8)\n",
    "\n",
    "for i in range(n_sites):\n",
    "    for j in range(n_sites):\n",
    "        if clabel[i, j] >=0:\n",
    "            text = plt.text(j, i, clabel[i, j],\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "plt.title('Cosine Similarities between documents')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-documentation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "KMeans_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-strike",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "websites_df[websites_df['cluster_label']==6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_clusters(fitted_model, inv_vocabulary, reducer = None, n_words = 10):\n",
    "    \"\"\"\n",
    "    Gets the first n most important words corresponding to each cluster centroid. \n",
    "    It needs the fitted KMeans object, and and inverse dictionary built from the vectorizer to look\n",
    "    for words by id.     \n",
    "    If dimensional reduction was performed, we also need the TSVD object used for dimensional reduction.\n",
    "    \n",
    "    Returns a pandas Dataframe with words and weights for each cluster.\n",
    "    \"\"\"\n",
    "    inv_vocab = inv_vocabulary\n",
    "    \n",
    "    if reducer is None:\n",
    "        centers = fitted_model.cluster_centers_\n",
    "    else:\n",
    "        centers = reducer.inverse_transform(fitted_model.cluster_centers_)\n",
    "    \n",
    "    #words = {}\n",
    "    #weights = {}\n",
    "    cluster_dict = {}\n",
    "    for i in range(centers.shape[0]):\n",
    "        vec = inversed_centers[i,:]\n",
    "        argsorted = np.argsort(vec)[::-1]\n",
    "\n",
    "        cluster_dict[f'Cluster_{i}'] = {'words': [inv_vocab[i] for i in argsorted[:n_words]], \n",
    "                                        'weights': vec[argsorted[:n_words]] }\n",
    "        \n",
    "        #words[f'Cluster_{i}'] = [inv_vocab[i] for i in argsorted[:n_words]]\n",
    "        #weights[f'Cluster_{i}'] = vec[argsorted[:n_words]] \n",
    "        \n",
    "        \n",
    "    return pd.DataFrame(cluster_dict).transpose()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-prospect",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = interpret_clusters(KMeans_model, inv_vocab, dim_reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-publication",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-province",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.xticks(rotation=45)\n",
    "plt.bar(cluster_df['words'][0], cluster_df['weights'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
