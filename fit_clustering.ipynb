{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-ukraine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering \n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import clustools as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-attachment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather files\n",
    "\n",
    "# Set the contents directory\n",
    "CONTENTS_DIR = './site_contents/'\n",
    "\n",
    "# Get all file names from the directory.\n",
    "\n",
    "file_names = [file for file in os.listdir(CONTENTS_DIR)]\n",
    "\n",
    "# Read the text of each file\n",
    "file_contents = []\n",
    "\n",
    "for name in file_names:\n",
    "    \n",
    "    with open(CONTENTS_DIR + name, 'r') as content:\n",
    "        site_text = content.read()\n",
    "    \n",
    "    file_contents.append(site_text)\n",
    "    \n",
    "# Store contents in dataframe\n",
    "\n",
    "websites_df = pd.DataFrame({'site': map(lambda name: name.replace('.txt','' ), file_names),\n",
    "                            'raw_text': file_contents})\n",
    "\n",
    "# Add column with split text, and one with the length of the split text.\n",
    "websites_df['wordcount'] = websites_df['raw_text'].apply(lambda mytext: len(mytext.split()))\n",
    "\n",
    "# Drop short or empty texts\n",
    "\n",
    "min_words = 100\n",
    "\n",
    "websites_df.drop(websites_df[websites_df['wordcount']< min_words].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-folder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IdF vectorization\n",
    "\n",
    "# get stop words list\n",
    "stop_words = ct.get_stopwords()\n",
    "\n",
    "# remove numbers and lowercase text\n",
    "my_preprocessor = ct.remove_numbers_lower\n",
    "\n",
    "# Select min and max doc frquency. \n",
    "\n",
    "min_freq = 2 # parameter to remove very uncommon words.\n",
    "max_freq = 0.4 # parameter to remove too common words.\n",
    "\n",
    "\n",
    "# Changed token patterns to keep words with {min_letters} or more only.\n",
    "\n",
    "min_letters = 3\n",
    "my_tokens = '(?u)\\\\b\\\\w{'+ str(min_letters -1) + '}\\\\w+\\\\b'\n",
    "\n",
    "#Create Vectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = min_freq,\n",
    "                             max_df = max_freq,\n",
    "                             preprocessor= my_preprocessor,\n",
    "                             stop_words=stop_words,\n",
    "                             token_pattern = my_tokens,\n",
    "                             ngram_range=(1,1)\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-lindsay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building vocabulary and generating the document term matrix (dtm).\n",
    "\n",
    "dtm = vectorizer.fit_transform(websites_df['raw_text'])\n",
    "\n",
    "# Build an inverse vocabulary dictionary to retrieve words easily by id. \n",
    "# A WARNING  appears due to some stopwords. To be fixed in the future.\n",
    "\n",
    "inv_vocab = {  w_id: word  for word, w_id in vectorizer.vocabulary_.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA. \n",
    "\n",
    "#Perform SVD acrros 20 dimensions and find an optimal number of dimensions to keep \n",
    "# Use scree plot elbow method. We could do up to n_docs, but it seems too much.\n",
    "\n",
    "# With a larger datasatet I'd check at least 100.\n",
    "\n",
    "# We need to use truncated SVD because we are dealing with an sparse matrix.\n",
    "# Create the SVD object. \n",
    "\n",
    "decomposer = TruncatedSVD(20, n_iter=10)\n",
    "\n",
    "# Perform SVD/LSA and get the transformed doc vectors. \n",
    "decomposer.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-matthew",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get singular values for scree plot.\n",
    "singular_values = decomposer.singular_values_\n",
    "\n",
    "dimension_number = np.array(list(range(len(singular_values))))\n",
    "\n",
    "# Get recomended number of dimensions to keep.\n",
    "\n",
    "elbow_idx = ct.ClusterDecissionHelper.elbow_finder(dimension_number, singular_values, plot = False )\n",
    "\n",
    "# Account for index starting at 0. I we keep up to and including dimension n (where elbow is)\n",
    "# we are keeping n+1 dimensions in total.\n",
    "dim_number = elbow_idx +1 \n",
    "\n",
    "print(f'Number of dimensions to keep according to scree plot: {dim_number} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get reduced vectors for the optimal number of dimensions found\n",
    "\n",
    "dim_reducer = TruncatedSVD(n_components = dim_number, n_iter= 10 )\n",
    "\n",
    "reduced_dtm =  dim_reducer.fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use decider to determine K via silhouette scores.\n",
    "# Check from 2 to 20 clusters, in 10 repetitions.\n",
    "\n",
    "decider = ct.ClusterDecissionHelper(max_clusters=20, repeat=10)\n",
    "\n",
    "decider.compute_scores(reduced_dtm, sil_score =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_K = decider.get_K_from_silhouettes()\n",
    "\n",
    "print(f'The best value of K found was K = {best_K} .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-packaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Clustering\n",
    "\n",
    "# Create a model for the best K\n",
    "\n",
    "KMeans_model = KMeans(n_clusters = best_K, n_init=20, max_iter= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the clusters\n",
    "KMeans_model.fit(reduced_dtm)\n",
    "\n",
    "# Write the labels into the dataframe.\n",
    "websites_df['cluster_label'] =  KMeans_model.labels_\n",
    "\n",
    "# How many elements in each cluster?\n",
    "websites_df.groupby('cluster_label')['site'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words and words weight representative of each cluster.\n",
    "\n",
    "cluster_df = ct.interpret_clusters(KMeans_model, inv_vocab, dim_reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "\n",
    "results_dir = './cluster_results/'\n",
    "\n",
    "timestr = time.strftime(\"%Y%m%d_%H%M\", time.localtime())\n",
    "\n",
    "# File names\n",
    "model_file = results_dir + timestr + '_fitted_kmeans_model.pickle'\n",
    "vectorizer_file = results_dir + timestr + '_fitted_tfidf.pickle'\n",
    "reducer_file = results_dir + timestr + '_fitted_TSVD.pickle'\n",
    "\n",
    "clustered_sites_file = results_dir + timestr +'_clustered_sites.csv'\n",
    "cluster_description_file = results_dir + timestr +'_cluster_descriptions.csv'\n",
    "\n",
    "# Save clustered sites and cluster descriptions.\n",
    "websites_df.sort_values('cluster_label').to_csv(clustered_sites_file ,columns=['site', 'cluster_label'])\n",
    "cluster_df.to_csv(cluster_description_file)\n",
    "\n",
    "# Save KMeans model, vectorizer and dim_reducer, which we will need for treating the data and \n",
    "# classifying later\n",
    "pickle.dump(KMeans_model, open(model_file, 'wb'))\n",
    "pickle.dump(vectorizer, open(vectorizer_file, 'wb'))\n",
    "pickle.dump(dim_reducer, open(reducer_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-processor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
