{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "arabic-recommendation",
   "metadata": {},
   "source": [
    "#  Web scrapping for NLP task.\n",
    "\n",
    "The goal of this notebook is to build a tool that can scrape text from a given list of websites, in order to use it later for clustering the sites. \n",
    "\n",
    "The task indicates that we should get text from the landing page, as well as text from the links contained in the landing page. \n",
    "\n",
    "Since many requests will be necessary, some mechanism has to be put in place in order to avoid being blocked. \n",
    "(user-agents, proxy, etc.)\n",
    "\n",
    "As each page contains many links, parallel processing can be implemented in order to speed up the scrapping. \n",
    "\n",
    "The final product should be able to take a list of websites and build text files with the contents of each site. \n",
    "Additional parameters could be included for managing, for instance, the pareallel processing, or maybe some further filtering of the contents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-links",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-subsection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_header():\n",
    "    \"\"\"\n",
    "    Returns a random header dictionary to be passed to requests. \n",
    "    \n",
    "    It'd be better to read the headers from an ecternal file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Headers for user agent rotation:\n",
    "    # Full headers obtained from hhttpbin.org\n",
    "    \n",
    "    \n",
    "    # Firefox 84 Ubuntu\n",
    "    h1 =  {\n",
    "        \"Accept\": \t\"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Encoding\":\t\"gzip, deflate, br\",\n",
    "        \"Accept-Language\":\t\"en-US,en;q=0.5\",\n",
    "        \"Connection\":\t\"keep-alive\",\n",
    "        \"Host\":\t\"httpbin.org\",\n",
    "        \"TE\":\t\"Trailers\",\n",
    "        \"Upgrade-Insecure-Requests\":\t\"1\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:84.0) Gecko/20100101 Firefox/84.0\"\n",
    "      }\n",
    "\n",
    "    #Firefox 84 Windows 10\n",
    "\n",
    "    h2 = {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\", \n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\", \n",
    "        \"Accept-Language\": \"en-GB,en;q=0.5\", \n",
    "        \"Host\": \"httpbin.org\", \n",
    "        \"Upgrade-Insecure-Requests\": \"1\", \n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:84.0) Gecko/20100101 Firefox/84.0\"\n",
    "       }\n",
    "\n",
    "    # Chrome 87 Ubuntu\n",
    "\n",
    "    h3 = {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\", \n",
    "        \"Accept-Language\": \"en-US,en;q=0.9,fr;q=0.8,es;q=0.7\", \n",
    "        \"Host\": \"httpbin.org\", \n",
    "        \"Sec-Fetch-Dest\": \"document\", \n",
    "        \"Sec-Fetch-Mode\": \"navigate\", \n",
    "        \"Sec-Fetch-Site\": \"none\", \n",
    "        \"Sec-Fetch-User\": \"?1\", \n",
    "        \"Upgrade-Insecure-Requests\": \"1\", \n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\", \n",
    "      }\n",
    "\n",
    "    #Chrome 87 Windows 10\n",
    "\n",
    "    h4 = {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "        \"Accept-Encoding\": \"gzip, deflate\", \n",
    "        \"Accept-Language\": \"es-419,es;q=0.9,fr;q=0.8,en;q=0.7\", \n",
    "        \"Host\": \"httpbin.org\", \n",
    "        \"Upgrade-Insecure-Requests\": \"1\", \n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36\"\n",
    "      }\n",
    "\n",
    "    # Microsoft Edge 87 Windows 10\n",
    "\n",
    "    h5 = {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\", \n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\", \n",
    "        \"Host\": \"httpbin.org\", \n",
    "        \"Sec-Fetch-Dest\": \"document\", \n",
    "        \"Sec-Fetch-Mode\": \"navigate\", \n",
    "        \"Sec-Fetch-Site\": \"none\", \n",
    "        \"Sec-Fetch-User\": \"?1\", \n",
    "        \"Upgrade-Insecure-Requests\": \"1\", \n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36 Edg/87.0.664.75\"\n",
    "      }\n",
    "\n",
    "    headers_list = [h1, h2, h3, h4, h5]\n",
    "    \n",
    "    return random.choice(headers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(soup):\n",
    "    \"\"\" Get get all the links for the given 'a' html tag from a parsed page.  \n",
    "    soup: an html page parsed with beautifulsoup.\n",
    "    \"\"\"\n",
    "    \n",
    "    links = [] # list to store the links found\n",
    "    \n",
    "    for link in soup.find_all('a', href=True):\n",
    "        links.append(link['href'])\n",
    "           \n",
    "    # avoiding repetitions\n",
    "    links = list(set(links))\n",
    "        \n",
    "        \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_links(home, links_list):\n",
    "    \"\"\"\n",
    "    Takes a home address and a list of links, and filters out links to external sites\n",
    "    and to some common file types.\n",
    "    home: string. The URL of the home page.\n",
    "    links_list: list of strings with the links found on the page, as produced by get_links.\n",
    "    \"\"\"\n",
    "    \n",
    "    domain = urlparse(home).netloc # domain to to check for external links.\n",
    "    \n",
    "    # path to include before an internal link. Remove final '/' if present.\n",
    "    path = home[:-1] if home.endswith('/') else home \n",
    "\n",
    "    unwanted_starts = ('javascript:', 'mailto:', 'tel:', '#', '..', '../', 'stream') \n",
    "    \n",
    "    unwanted_endings = ('.pdf', '.jpg', '.jpeg', '.png', '.gif', '.exe', '.js',\n",
    "                        '.zip', '.tar', '.gz', '.7z', '.rar'\n",
    "                       )\n",
    "    \n",
    "    filtered_links = list(filter(lambda link: not (link.lower().startswith(unwanted_starts) or \n",
    "                                                   link.lower().endswith(unwanted_endings)),links_list\n",
    "                                )\n",
    "                         )\n",
    "    \n",
    "    # get internal links that don't have the full URL\n",
    "    internal_links = [link for link in filtered_links if not link.startswith('http') ]\n",
    "\n",
    "    # Ensure starting '/'  \n",
    "    for j, intlink in enumerate(internal_links):\n",
    "        if not intlink.startswith('/'):\n",
    "            internal_links[j]='/'+intlink\n",
    "            \n",
    "    internal_links = [path + intlink for intlink in internal_links]\n",
    "    \n",
    "    # removing external links\n",
    "    filtered_links = list(filter(lambda link: (link.lower().startswith('http') and\n",
    "                                                domain in link.lower()), filtered_links\n",
    "                                )\n",
    "                         )\n",
    "    \n",
    "    # include internal links\n",
    "    filtered_links.extend(internal_links)\n",
    "    \n",
    "    # keeping disntinct elements only\n",
    "    \n",
    "    filtered_links = list(set(filtered_links))\n",
    "    \n",
    "    # remove home url if present.    \n",
    "    try:\n",
    "        filtered_links.remove(path)\n",
    "    except(ValueError):\n",
    "        pass\n",
    "    try:\n",
    "        filtered_links.remove(path+'/')\n",
    "    except(ValueError):\n",
    "        pass\n",
    "        \n",
    "    return filtered_links\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnwantedContentError(Exception):\n",
    "     \"\"\" Raised if the content of the site was not text \"\"\"\n",
    "    \n",
    "    def __init__(self, content_type):\n",
    "        self.content_type = content_type\n",
    "    \n",
    "    def message(self):\n",
    "        \n",
    "        msg = f'Unwanted content type:  {self.content_type} .'\n",
    "        \n",
    "        return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_main(main_url):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes the URL of the main site, scrapes the text and the links. \n",
    "    site_url: string. url of the desired site.\n",
    "    \"\"\"\n",
    "    \n",
    "    random_header = get_header()\n",
    "    \n",
    "    page = requests.get(main_url, {'header': random_header}, timeout=(3, 5))\n",
    "    \n",
    "    # Verify we didn't get and invalid response.\n",
    "    page.raise_for_status()\n",
    "    \n",
    "    # Verify content type is text.\n",
    "    if not page.headers['Content-Type'].startswith('text'):\n",
    "        raise UnwantedContentError(page.headers['Content-Type'])\n",
    "    else:    \n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "        page_text = soup.get_text(separator = '\\n', strip=True) \n",
    "\n",
    "        page_links = get_links(soup)\n",
    "        page_links = filter_links(main_url, page_links)\n",
    "\n",
    "        return page_text, page_links   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(link_url):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes the URL from one of the link, scrapesa and returns the text. \n",
    "    link_url: string. url of the desired site.\n",
    "    \"\"\"\n",
    "    \n",
    "    random_header = get_header()\n",
    "    \n",
    "      \n",
    "    page = requests.get(link_url, params = {'header': random_header}, timeout=(5, 5))\n",
    "    \n",
    "    # Verify we didn't get and invalid response. \n",
    "    page.raise_for_status()    \n",
    "    \n",
    "    # Verify content type is text.\n",
    "    if not page.headers['Content-Type'].startswith('text'):\n",
    "        raise UnwantedContentError(page.headers['Content-Type'])\n",
    "    else:\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        page_text = soup.get_text(separator = '\\n', strip=True) \n",
    "\n",
    "        return page_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links_try(mylink_url):\n",
    "    \"\"\"\n",
    "    Scrape links and catch exceptions. This is a wrapper function intended to be passed\n",
    "    to a multiplrocessing.Pool object.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        scrapping_result = scrape_links(mylink_url)\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        scrapping_result = [mylink_url, err]\n",
    "    except UnwantedContentError as err:\n",
    "        scrapping_result = [mylink_url, err.message()]\n",
    "        print(f'WARNING: Content error in \\n {mylink_url} \\n {scrapping_result[1]} ')\n",
    "        \n",
    "    return scrapping_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_full(sites_list, contents_dir = None, logs_dir = None, max_links = 20, n_process = None ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to scrape all sites and links on the main page, extracting the text of each\n",
    "    site to a text file.\n",
    "    Creates a log file for the scrapping as a whole. Creates a link scrapping report for sites\n",
    "    where more than half the links failed. \n",
    "    Returns a report_dict with the main information of the scrapping.\n",
    "    \n",
    "    sites_list: list. A list of sites urls in string format.\n",
    "    contents_dir: string. Directory to store the text files with the contents. If None, a \n",
    "                  default directory is used. \n",
    "    logs_dir: string. Directory to store the text files with the scrapping logs. If None, a \n",
    "                  default directory is used. \n",
    "    max_links: int. Maximum number of links to take from each site. Note: Some of the links\n",
    "               may not have scrappable contents. \n",
    "    n_process: int. Number of processes to be passed to Pool for performing the parallell scraping \n",
    "               of each site's links. If None, os.cpu_count() is used. \n",
    "               WARNING: Setting n_process greater thant os.cpu_count() may cause crash.\n",
    "               A check for this will be included soon. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Setting default directories if none are provided.\n",
    "    if contents_dir is None:\n",
    "        contents_dir = './site_contents/'\n",
    "    if logs_dir is None:        \n",
    "        logs_dir = './logs/'\n",
    "\n",
    "    # Make contents and logs directories if they don't exist.\n",
    "    os.makedirs(contents_dir, exist_ok=True)\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "    # Keep track of the result of each request. \n",
    "    failed_requests = []\n",
    "    succes_requests = []\n",
    "\n",
    "    # Time string to identify the corresponding log. \n",
    "    timestr = time.strftime(\"%Y%m%d_%H%M\", time.localtime())\n",
    "    \n",
    "    start_time = time.time() # save for timing\n",
    "\n",
    "    for site in sites_list:\n",
    "\n",
    "        # Get links and text from the main site. Catch request errors and UnwantedContents. \n",
    "\n",
    "        try:\n",
    "            text, links_list = scrape_main(site)\n",
    "            succes_requests.append(site)\n",
    "        except requests.exceptions.RequestException as err:\n",
    "            failed_requests.append((site,str(err)))\n",
    "            print(f'Error in site: {site}')\n",
    "        except UnwantedContentError as err:\n",
    "            failed_requests.append((site,err.message()))\n",
    "        else:   \n",
    "\n",
    "            # Create site contents file and write text of the main site\n",
    "            domain = urlparse(site).netloc\n",
    "            file_name = contents_dir + domain +'.txt'\n",
    "\n",
    "            with open(file_name, 'w') as f:\n",
    "                f.write(text)\n",
    "\n",
    "            print(f'Text from main page {domain} written to {file_name}')\n",
    "\n",
    "            start_time_p = time.time()\n",
    "            print(f'Scrapping links from {domain}')\n",
    "\n",
    "            links_list = links_list[:max_links] # Truncate link list.\n",
    "            \n",
    "            # Use multiprocessing pool for scrapping links.\n",
    "            with Pool(n_process) as p:\n",
    "                link_scrap_results = p.map(scrape_links_try, links_list)\n",
    "\n",
    "            duration_p = time.time() - start_time_p\n",
    "\n",
    "            print(f'{len(links_list)} links scrapped in {duration_p:.2f} seconds. ')\n",
    "            \n",
    "            \n",
    "            # Separte link's text from failed links.\n",
    "            text = list(filter(lambda result: type(result) is str,link_scrap_results))\n",
    "            link_errors = list(filter(lambda result: type(result) is list,link_scrap_results))\n",
    "            link_errors = [[site, str(err)] for site, err in link_errors]\n",
    "\n",
    "            print(f'{len(link_errors)} links failed.')\n",
    "\n",
    "            # Write link text to site file.\n",
    "            with open(file_name, 'a') as f:\n",
    "                f.write('\\n'.join(text))\n",
    "\n",
    "            # Log link errors only if more than half fail.\n",
    "            if len(link_errors) >= len(link_scrap_results)/2 : \n",
    "\n",
    "                link_errors = ['\\n'.join(error) for error in link_errors]\n",
    "\n",
    "                with open(logs_dir+timestr+ '_link_report_' + domain+'.txt', 'w') as link_log:\n",
    "\n",
    "                        link_log.write('\\n\\n'.join(link_errors))\n",
    "                        \n",
    "    duration = time.time()-start_time\n",
    "    \n",
    "    report_dict = {'sites': sites_list, \n",
    "                   'succesful': succes_requests,\n",
    "                   'failed': failed_requests,\n",
    "                   'time_s': duration,\n",
    "                   'contents' : contents_dir,\n",
    "                   'logs': logs_dir,\n",
    "                   'report_name': logs_dir+timestr+'_scrapping_repport'\n",
    "                  }\n",
    "\n",
    "    report_str = (f'{len(sites_list) } requested. \\n' \n",
    "                  + f'Scrapping took {duration/60:.2f} min ({duration:.2f} s) \\n' \n",
    "                  + f'{len(succes_requests) } SUCCESFUL. \\n'\n",
    "                  + f'{len(failed_requests) } FAILURES. \\n'\n",
    "                  + '='*20 +'\\n FAILED SITES \\n' + '='*20 + '\\n\\n')\n",
    "\n",
    "\n",
    "    with open(report_dict['report_name'], 'w') as scrapping_log:\n",
    "\n",
    "        scrapping_log.write(report_str)\n",
    "        scrapping_log.writelines(['\\n'.join(failure)+'\\n\\n' for failure in failed_requests])\n",
    "\n",
    "        scrapping_log.write('\\n\\n SUCCESFUL REQUESTS \\n\\n')\n",
    "        scrapping_log.write('\\n'.join(succes_requests))\n",
    "        \n",
    "    return report_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-departure",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    SITES_LIST = []\n",
    "\n",
    "    with open('./site_lists/01_websites.csv', 'r', newline = '') as f:\n",
    "        for site in f.readlines():\n",
    "            SITES_LIST.append(site.strip())\n",
    "\n",
    "    print(f'The list contains {len(SITES_LIST)} sites.')\n",
    "    \n",
    "    report = scrape_full(SITES_LIST)\n",
    "    \n",
    "    print('='*20 + '\\n Scrapping Summary \\n' +'='*20 +'\\n' )\n",
    "    print(f'{len(report[\"sites\"])} sites requested. \\n' \n",
    "          + f'Scrapping took {report[\"time_s\"]/60:.2f} min ({report[\"time_s\"]:.2f} s) \\n' \n",
    "          + f'{len(report[\"succesful\"]) } SUCCESFUL. \\n'\n",
    "          + f'{len(report[\"failed\"]) } FAILURES. \\n\\n'\n",
    "          + f'Contents in: {report[\"contents\"]} \\n'\n",
    "          + f'Logs in: {report[\"logs\"]} \\n'\n",
    "          + f'Full report: {report[\"report_name\"]}' \n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-listening",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
