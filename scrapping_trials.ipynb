{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "buried-particular",
   "metadata": {},
   "source": [
    "#  Web scrapping for NLP task.\n",
    "\n",
    "The goal of this notebook is to build a tool that can scrape text from a given list of websites, in order to use it later for clustering the sites. \n",
    "\n",
    "The task indicates that we should get text from the landing page, as well as text from the links contained in the landing page. \n",
    "\n",
    "Since many requests will be necessary, some mechanism has to be put in place in order to avoid being blocked. \n",
    "(user-agents, proxy, etc.)\n",
    "\n",
    "As each page contains many links, parallel processing can be implemented in order to speed up the scrapping. \n",
    "\n",
    "The final product should be able to take a list of websites and build text files with the contents of each site. \n",
    "Additional parameters could be included for managing, for instance, the pareallel processing, or maybe some further filtering of the contents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-henry",
   "metadata": {},
   "source": [
    "##  Scrapping from one site\n",
    "\n",
    "Let's use one of the given URLs to get an idea of the kind of websites we have. \n",
    "\n",
    "For example, this british pipe supplier: http://www.besseges-vtf.co.uk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-heart",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-correction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_header():\n",
    "    \"\"\"\n",
    "    Returns a random header dictionary to be passed to requests. \n",
    "    \"\"\"\n",
    "\n",
    "    # Headers for user agent rotation:\n",
    "    # Full headers obtained from hhttpbin.org\n",
    "    # Firefox 84 Ubuntu\n",
    "\n",
    "\n",
    "    h1 =  {\n",
    "        \"Accept\": \t\"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Encoding\":\t\"gzip, deflate, br\",\n",
    "        \"Accept-Language\":\t\"en-US,en;q=0.5\",\n",
    "        \"Connection\":\t\"keep-alive\",\n",
    "        \"Host\":\t\"httpbin.org\",\n",
    "        \"TE\":\t\"Trailers\",\n",
    "        \"Upgrade-Insecure-Requests\":\t\"1\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:84.0) Gecko/20100101 Firefox/84.0\"\n",
    "      }\n",
    "\n",
    "    #Firefox 84 Windows 10\n",
    "\n",
    "    h2 = {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\", \n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\", \n",
    "        \"Accept-Language\": \"en-GB,en;q=0.5\", \n",
    "        \"Host\": \"httpbin.org\", \n",
    "        \"Upgrade-Insecure-Requests\": \"1\", \n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:84.0) Gecko/20100101 Firefox/84.0\"\n",
    "       }\n",
    "\n",
    "    # Chrome 87 Ubuntu\n",
    "\n",
    "    h3 = {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\", \n",
    "        \"Accept-Language\": \"en-US,en;q=0.9,fr;q=0.8,es;q=0.7\", \n",
    "        \"Host\": \"httpbin.org\", \n",
    "        \"Sec-Fetch-Dest\": \"document\", \n",
    "        \"Sec-Fetch-Mode\": \"navigate\", \n",
    "        \"Sec-Fetch-Site\": \"none\", \n",
    "        \"Sec-Fetch-User\": \"?1\", \n",
    "        \"Upgrade-Insecure-Requests\": \"1\", \n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\", \n",
    "      }\n",
    "\n",
    "    #Chrome 87 Windows 10\n",
    "\n",
    "    h4 = {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "        \"Accept-Encoding\": \"gzip, deflate\", \n",
    "        \"Accept-Language\": \"es-419,es;q=0.9,fr;q=0.8,en;q=0.7\", \n",
    "        \"Host\": \"httpbin.org\", \n",
    "        \"Upgrade-Insecure-Requests\": \"1\", \n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36\"\n",
    "      }\n",
    "\n",
    "    # Microsoft Edge 87 Windows 10\n",
    "\n",
    "    h5 = {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\", \n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\", \n",
    "        \"Host\": \"httpbin.org\", \n",
    "        \"Sec-Fetch-Dest\": \"document\", \n",
    "        \"Sec-Fetch-Mode\": \"navigate\", \n",
    "        \"Sec-Fetch-Site\": \"none\", \n",
    "        \"Sec-Fetch-User\": \"?1\", \n",
    "        \"Upgrade-Insecure-Requests\": \"1\", \n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36 Edg/87.0.664.75\"\n",
    "      }\n",
    "\n",
    "    headers_list = [h1, h2, h3, h4, h5]\n",
    "    \n",
    "    return random.choice(headers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(soup, tags = 'a'):\n",
    "    \"\"\" Get get all the links for the given tags from a parsed page.  \n",
    "    soup: an html page parsed with beautifulsoup.\n",
    "    tags: string or list of strings indicating the html tags to search. \n",
    "    \"\"\"\n",
    "    \n",
    "    links = [] # list to store the links found\n",
    "    \n",
    "    for tag in tags:\n",
    "        for link in soup.find_all(tag, href=True):\n",
    "            links.append(link['href'])\n",
    "           \n",
    "    # avoiding repetitions\n",
    "    links = list(set(links))\n",
    "        \n",
    "        \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_links(home, links_list):\n",
    "    \"\"\"\n",
    "    Takes a home address and a list of links, and filters out links to external sites\n",
    "    and to some common file types.\n",
    "    home: string. The URL of the home page.\n",
    "    links_list: list of strings with the links found on the page, as produced by get_links.\n",
    "    \"\"\"\n",
    "    \n",
    "    domain = urlparse(home).netloc # domain to to check for external links.\n",
    "    \n",
    "    # path to include before an internal link. Remove final '/' if present.\n",
    "    path = home[:-1] if home.endswith('/') else home \n",
    "\n",
    "    unwanted_starts = ('javascript:', 'mailto:', 'tel:', '#', '..', '../') \n",
    "    \n",
    "    unwanted_endings = ('.pdf', '.jpg', '.jpeg', '.png', '.gif', '.exe', '.js',\n",
    "                        '.zip', '.tar', '.gz', '.7z', '.rar'\n",
    "                       )\n",
    "    \n",
    "    filtered_links = list(filter(lambda link: not (link.lower().startswith(unwanted_starts) or \n",
    "                                                   link.lower().endswith(unwanted_endings)),links_list\n",
    "                                )\n",
    "                         )\n",
    "    \n",
    "    # get internal links that don't have the full URL\n",
    "    internal_links = [link for link in filtered_links if not link.startswith('http') ]\n",
    "\n",
    "    # Ensure starting '/'  \n",
    "    for j, intlink in enumerate(internal_links):\n",
    "        if not intlink.startswith('/'):\n",
    "            internal_links[j]='/'+intlink\n",
    "            \n",
    "    internal_links = [path + intlink for intlink in internal_links]\n",
    "    \n",
    "    # removing external links\n",
    "    filtered_links = list(filter(lambda link: (link.lower().startswith('http') and\n",
    "                                                domain in link.lower()), filtered_links\n",
    "                                )\n",
    "                         )\n",
    "    \n",
    "    # include internal links\n",
    "    filtered_links.extend(internal_links)\n",
    "    \n",
    "    # keeping disntinct elements only\n",
    "    \n",
    "    filtered_links = list(set(filtered_links))\n",
    "    \n",
    "    # remove home url if present.    \n",
    "    try:\n",
    "        filtered_links.remove(path)\n",
    "    except(ValueError):\n",
    "        pass\n",
    "    try:\n",
    "        filtered_links.remove(path+'/')\n",
    "    except(ValueError):\n",
    "        pass\n",
    "        \n",
    "    return filtered_links\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_main(main_url):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes the URL of the main site, scrapes the text and the links. \n",
    "    site_url: string. url of the desired site.\n",
    "    \"\"\"\n",
    "    \n",
    "    random_header = get_header()\n",
    "    \n",
    "    page = requests.get(main_url, {'header': random_header}, timeout=(2, 5))\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    \n",
    "    page_text = soup.get_text(separator = '\\n', strip=True) \n",
    "       \n",
    "    page_links = get_links(soup)\n",
    "    page_links = filter_links(main_url, page_links)\n",
    "        \n",
    "    return page_text, page_links   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(link_url):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes the URL from one of the link, scrapesa and returns the text. \n",
    "    link_url: string. url of the desired site.\n",
    "    \"\"\"\n",
    "    \n",
    "    random_header = get_header()\n",
    "    \n",
    "      \n",
    "    page = requests.get(link_url, params = {'header': random_header}, timeout=(2, 5))\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "    page_text = soup.get_text(separator = '\\n', strip=True) \n",
    "    \n",
    "    #print(f'Retrieved text from {link_url}')\n",
    "    \n",
    "    return page_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links_try(mylink_url):\n",
    "    \n",
    "    try:\n",
    "        scrapping_result = scrape_links(mylink_url)\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        scrapping_result = [mylink_url, err]\n",
    "        \n",
    "    return scrapping_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-redhead",
   "metadata": {},
   "source": [
    "Trial URLs\n",
    "\n",
    "    *'http://www.besseges-vtf.co.uk'\n",
    "    *'http://lumaquin.com'\n",
    "    *'https://www.degso.com'\n",
    "    *'http://www.ictsl.net'\n",
    "    *'https://barrocorestaurante.mx'\n",
    "    *'https://www.gummigoetz.de'\n",
    "    *'http://www.suppliersof.com'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "SITES_LIST = []\n",
    "\n",
    "with open('./site_lists/01_websites.csv', 'r', newline = '') as f:\n",
    "    for site in f.readlines():\n",
    "        SITES_LIST.append(site.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-finland",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MAX_LINKS = 20\n",
    "\n",
    "CONTENTS_DIR = './site_contents/'\n",
    "LOG_DIR = './logs/'\n",
    "\n",
    "failed_requests = []\n",
    "succes_requests = []\n",
    "\n",
    "for site in SITES_LIST:\n",
    "    \n",
    "    domain = urlparse(site).netloc\n",
    "    \n",
    "    start_time = time.localtime()\n",
    "    timestr = time.strftime(\"%Y%m%d_%H%M\", start_time)\n",
    "    \n",
    "    file_name = CONTENTS_DIR + domain\n",
    "\n",
    "    # get links and text from the main site\n",
    "\n",
    "    try:\n",
    "        text, links_list = scrape_main(site)\n",
    "        succes_requests.append(site)\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        failed_requests.append((site,str(err)))\n",
    "        print(f'Error in site: {site}')\n",
    "    else:   \n",
    "        \n",
    "        # Create file and write text of the main site\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write(text)\n",
    "\n",
    "        print(f'Text from main page {domain} written to {file_name}')\n",
    "          \n",
    "        start_time_p = time.time()\n",
    "        print('BEGIN PARALLELL LINK SCRAPPING')\n",
    "        \n",
    "        links_list = links_list[:MAX_LINKS]\n",
    "        if __name__ == '__main__':\n",
    "            with Pool(6) as p:\n",
    "                link_scrap_results = p.map(scrape_links_try, links_list)\n",
    "                       \n",
    "        duration_p = time.time() - start_time_p\n",
    "        \n",
    "        print(f'{len(links_list)} links scrapped in {duration_p:.2f} seconds. ')\n",
    "\n",
    "\n",
    "        text = list(filter(lambda result: type(result) is str,link_scrap_results))\n",
    "        link_errors = list(filter(lambda result: type(result) is list,link_scrap_results))\n",
    "        link_errors = [[site, str(err)] for site, err in link_errors]\n",
    "        \n",
    "        \n",
    "        with open(file_name, 'a') as f:\n",
    "            f.write('\\n'.join(text))\n",
    "        \n",
    "        # Log link errors if more than half fail.\n",
    "        if len(link_errors) >= len(link_scrap_results)/2 : \n",
    "\n",
    "            link_errors = ['\\n'.join(error) for error in link_errors]\n",
    "\n",
    "            with open(LOG_DIR+timestr+ '_link_report_' + domain, 'w') as link_log:\n",
    "\n",
    "                    link_log.writelines(link_errors)\n",
    "\n",
    "report_str = (f'{len(SITES_LIST) } requested.' + '\\n' \n",
    "              + f'{len(succes_requests) } SUCCESFUL.' + '\\n'\n",
    "              + f'{len(failed_requests) } FAILURES.' + '\\n'\n",
    "              + '='*20 +'\\n FAILED SITES \\n' + '='*20 + '\\n\\n')\n",
    "                \n",
    "               \n",
    "with open(LOG_DIR+timestr+'_scrapping_repport', 'w') as scrapping_log:\n",
    "    \n",
    "    scrapping_log.write(report_str)\n",
    "    scrapping_log.writelines(['\\n'.join(failure)+'\\n\\n' for failure in failed_requests])\n",
    "    \n",
    "    scrapping_log.write('\\n\\n SUCCESFUL REQUESTS \\n\\n')\n",
    "    scrapping_log.write('\\n'.join(succes_requests))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-language",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
