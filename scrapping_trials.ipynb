{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "gothic-alert",
   "metadata": {},
   "source": [
    "#  Web scrapping for NLP task.\n",
    "\n",
    "The goal of this notebook is to build a tool that can scrape text from a given list of websites, in order to use it later for clustering the sites. \n",
    "\n",
    "The task indicates that we should get text from the landing page, as well as text from the links contained in the landing page. \n",
    "\n",
    "Since many requests will be necessary, some mechanism has to be put in place in order to avoid being blocked. \n",
    "(user-agents, proxy, etc.)\n",
    "\n",
    "As each page contains many links, parallel processing can be implemented in order to speed up the scrapping. \n",
    "\n",
    "The final product should be able to take a list of websites and build text files with the contents of each site. \n",
    "Additional parameters could be included for managing, for instance, the pareallel processing, or maybe some further filtering of the contents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-pepper",
   "metadata": {},
   "source": [
    "##  Scrapping from one site\n",
    "\n",
    "Let's use one of the given URLs to get an idea of the kind of websites we have. \n",
    "\n",
    "For example, this british pipe supplier: http://www.besseges-vtf.co.uk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers for user agent rotation:\n",
    "# Full headers obtained from hhttpbin.org\n",
    "\n",
    "# Firefox 84 Ubuntu\n",
    "\n",
    "h1 = {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Encoding\":\"gzip, deflate, br\",\n",
    "    \"Accept-Language\":\"en-US,en;q=0.5\",\n",
    "    \"Connection\":\"keep-alive\",\n",
    "    \"Host\":\"httpbin.org\",\n",
    "    \"TE\":\"Trailers\",\n",
    "    \"Upgrade-Insecure-Requests\":\"1\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:84.0) Gecko/20100101 Firefox/84.0\"\n",
    "  }\n",
    "# Chrome 87 Ubuntu\n",
    "h2= {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\", \n",
    "    \"Accept-Language\": \"en-US,en;q=0.9,fr;q=0.8,es;q=0.7\", \n",
    "    \"Host\": \"httpbin.org\", \n",
    "    \"Sec-Fetch-Dest\": \"document\", \n",
    "    \"Sec-Fetch-Mode\": \"navigate\", \n",
    "    \"Sec-Fetch-Site\": \"none\", \n",
    "    \"Sec-Fetch-User\": \"?1\", \n",
    "    \"Upgrade-Insecure-Requests\": \"1\", \n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\", \n",
    "  }\n",
    "\n",
    "headers_list = [h1,h2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-solid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(soup, tags = 'a', remove_duplicates = True):\n",
    "    \"\"\" Get get all the links for the given tags from a parsed page.  \n",
    "    soup: an html page parsed with beautifulsoup.\n",
    "    tags: string or list of strings indicating the html tags to search. \n",
    "    \"\"\"\n",
    "    \n",
    "    links = [] # list to store the links found\n",
    "    \n",
    "    for tag in tags:\n",
    "        for link in soup.find_all(tag, href=True):\n",
    "            links.append(link['href'])\n",
    "           \n",
    "    #remove repeated elements\n",
    "    if remove_duplicates:\n",
    "        links = list(set(links))\n",
    "    \n",
    "    #links = [link['href'] for tag in tags for link in soup.find_all(tag, href=True) ]        \n",
    "        \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_links(home, links_list):\n",
    "    \"\"\"\n",
    "    Takes a home address and a list of links, and filters out links to external sites\n",
    "    and to some common file types.\n",
    "    home: string. The URL of the home page.\n",
    "    links_list: list of strings with the links found on the page, as produced by get_links.\n",
    "    \"\"\"\n",
    "    \n",
    "    domain = urlparse(home).netloc # domain to to check for external links.\n",
    "    \n",
    "    # path to include before an internal link. Remove final '/' if present.\n",
    "    path = home[:-1] if home.endswith('/') else home \n",
    "\n",
    "    unwanted_starts = ('javascript:', 'mailto:', 'tel:', '#', '..', '../') \n",
    "    \n",
    "    unwanted_endings = ('.pdf', '.jpg', '.jpeg', '.png', '.gif', '.exe', '.js',\n",
    "                        '.zip', '.tar', '.gz', '.7z', '.rar'\n",
    "                       )\n",
    "    \n",
    "    filtered_links = list(filter(lambda link: not (link.lower().startswith(unwanted_starts) or \n",
    "                                                   link.lower().endswith(unwanted_endings)),links_list\n",
    "                                )\n",
    "                         )\n",
    "    \n",
    "    # get internal links that don't have the full URL\n",
    "    internal_links = [link for link in filtered_links if not link.startswith('http') ]\n",
    "\n",
    "    # Ensure starting '/'  \n",
    "    for j, intlink in enumerate(internal_links):\n",
    "        if not intlink.startswith('/'):\n",
    "            internal_links[j]='/'+intlink\n",
    "            \n",
    "    internal_links = [path + intlink for intlink in internal_links]\n",
    "    \n",
    "    # removing external links\n",
    "    filtered_links = list(filter(lambda link: (link.lower().startswith('http') and\n",
    "                                                domain in link.lower()), filtered_links\n",
    "                                )\n",
    "                         )\n",
    "    \n",
    "    # include internal links\n",
    "    filtered_links.extend(internal_links)\n",
    "    \n",
    "    # remove home url if present.    \n",
    "    try:\n",
    "        filtered_links.remove(path)\n",
    "    except(ValueError):\n",
    "        pass\n",
    "    try:\n",
    "        filtered_links.remove(path+'/')\n",
    "    except(ValueError):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    return filtered_links\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_page(file, text):\n",
    "    \n",
    "    with open(file,'a') as websitetext:\n",
    "        websitetext.write(text)\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-guinea",
   "metadata": {},
   "source": [
    "Trial URLs\n",
    "\n",
    "    *'http://www.besseges-vtf.co.uk'\n",
    "    *'http://lumaquin.com'\n",
    "    *'https://www.degso.com'\n",
    "    *'http://www.ictsl.net'\n",
    "    *'https://barrocorestaurante.mx'\n",
    "    *'https://www.gummigoetz.de'\n",
    "    *'http://www.suppliersof.com'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set landing page URL\n",
    "\n",
    "MAIN_URL = 'http://www.besseges-vtf.co.uk'\n",
    "\n",
    "FILES_DIRECTORY = './site_contents/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request contents\n",
    "\n",
    "random_header = random.choice(headers_list)\n",
    "\n",
    "landing_page = requests.get(MAIN_URL, {'header': random_header})\n",
    "landing_html = BeautifulSoup(landing_page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-money",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a file name for the website and write the text of the main page.\n",
    "file_name = FILES_DIRECTORY + urlparse(MAIN_URL).netloc\n",
    "\n",
    "landing_page_text=landing_html.get_text(separator = '\\n', strip=True)\n",
    "\n",
    "write_page(file_name, landing_page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-agreement",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = get_links(landing_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-vermont",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = filter_links(MAIN_URL, link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-fleet",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
